{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git lfs install"
      ],
      "metadata": {
        "id": "Oo46qAbVANjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYdtPhoC_vfU"
      },
      "outputs": [],
      "source": [
        "!git clone https://huggingface.co/datasets/sshao0516/CrowdHuman"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile, os\n",
        "\n",
        "zip_files = [\n",
        "    \"/content/CrowdHuman/CrowdHuman_train01.zip\",\n",
        "    \"/content/CrowdHuman/CrowdHuman_train02.zip\",\n",
        "    \"/content/CrowdHuman/CrowdHuman_train03.zip\",\n",
        "    \"/content/CrowdHuman/CrowdHuman_val.zip\"\n",
        "]\n",
        "extract_dir = \"/content/CrowdHuman/images\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "for zf in zip_files:\n",
        "    with zipfile.ZipFile(zf, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n"
      ],
      "metadata": {
        "id": "Bpu5lEHIE8Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# STEP 2: Load IDs\n",
        "def load_image_ids_from_odgt(odgt_path):\n",
        "    ids = []\n",
        "    with open(odgt_path, 'r') as f:\n",
        "        for line in f:\n",
        "            data = json.loads(line.strip())\n",
        "            ids.append(data['ID'])\n",
        "    return ids\n",
        "\n",
        "train_ids_all = load_image_ids_from_odgt(\"/content/CrowdHuman/annotation_train.odgt\")\n",
        "val_ids_all = load_image_ids_from_odgt(\"/content/CrowdHuman/annotation_val.odgt\")\n",
        "\n",
        "print(\"Train IDs (Full):\", len(train_ids_all))\n",
        "print(\"Val IDs (Full):\", len(val_ids_all))"
      ],
      "metadata": {
        "id": "7Y4dDJr-q6Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, glob\n",
        "from PIL import Image\n",
        "\n",
        "input_ann = \"/content/CrowdHuman/annotation_train.odgt\"\n",
        "img_dir = \"/content/CrowdHuman/images/Images\"\n",
        "yolo_labels = \"/content/CrowdHuman/labels/train\"\n",
        "os.makedirs(yolo_labels, exist_ok=True)\n",
        "\n",
        "with open(input_ann, \"r\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        img_name = data[\"ID\"] + \".jpg\"\n",
        "        img_path = os.path.join(img_dir, img_name)\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            continue\n",
        "\n",
        "        img = Image.open(img_path)\n",
        "        W, H = img.size\n",
        "        label_path = os.path.join(yolo_labels, data[\"ID\"] + \".txt\")\n",
        "\n",
        "        with open(label_path, \"w\") as lf:\n",
        "            for gt in data[\"gtboxes\"]:\n",
        "                if gt[\"tag\"] != \"person\":\n",
        "                    continue\n",
        "                x1, y1, w, h = gt[\"fbox\"]   # full body box\n",
        "                xc = (x1 + w/2) / W\n",
        "                yc = (y1 + h/2) / H\n",
        "                wf = w / W\n",
        "                hf = h / H\n",
        "                lf.write(f\"0 {xc} {yc} {wf} {hf}\\n\")\n"
      ],
      "metadata": {
        "id": "lw0vRbGME-Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, glob\n",
        "from PIL import Image\n",
        "\n",
        "input_ann = \"/content/CrowdHuman/annotation_val.odgt\"\n",
        "img_dir = \"/content/CrowdHuman/images/Images\"\n",
        "yolo_labels = \"/content/CrowdHuman/labels/test\"\n",
        "os.makedirs(yolo_labels, exist_ok=True)\n",
        "\n",
        "with open(input_ann, \"r\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        img_name = data[\"ID\"] + \".jpg\"\n",
        "        img_path = os.path.join(img_dir, img_name)\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            continue\n",
        "\n",
        "        img = Image.open(img_path)\n",
        "        W, H = img.size\n",
        "        label_path = os.path.join(yolo_labels, data[\"ID\"] + \".txt\")\n",
        "\n",
        "        with open(label_path, \"w\") as lf:\n",
        "            for gt in data[\"gtboxes\"]:\n",
        "                if gt[\"tag\"] != \"person\":\n",
        "                    continue\n",
        "                x1, y1, w, h = gt[\"fbox\"]   # full body box\n",
        "                xc = (x1 + w/2) / W\n",
        "                yc = (y1 + h/2) / H\n",
        "                wf = w / W\n",
        "                hf = h / H\n",
        "                lf.write(f\"0 {xc} {yc} {wf} {hf}\\n\")"
      ],
      "metadata": {
        "id": "j68NUPsWsdn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6 min"
      ],
      "metadata": {
        "id": "M_30WETmjs5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install fiftyone\n",
        "import fiftyone.zoo as foz\n",
        "import fiftyone as fo\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"open-images-v6\",\n",
        "    split=\"train\",\n",
        "    label_types=[\"detections\"],\n",
        "    classes=[\"Gun\",\"Knife\",\"Rifle\"],\n",
        "    max_samples=5000\n",
        ")"
      ],
      "metadata": {
        "id": "OuSUuAPEFALu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fiftyone as fo\n",
        "\n",
        "dataset.export(\n",
        "    export_dir=\"/content/openimages_weapon\",\n",
        "    dataset_type=fo.types.COCODetectionDataset,\n",
        "    label_field=\"ground_truth\"  # <-- change from \"detections\" to \"ground_truth\"\n",
        ")"
      ],
      "metadata": {
        "id": "BDoxsKO8FDHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "y3G5LaVjommU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import yaml\n",
        "import glob\n",
        "\n",
        "def prepare_dataset(root_images, root_labels, output_dir, split_ratio=0.8):\n",
        "    \"\"\"\n",
        "    root_images: folder with all images (e.g., /content/CrowdHuman/images/Images)\n",
        "    root_labels: folder with all YOLO labels (train folder)\n",
        "    output_dir: folder to create train/val subfolders\n",
        "    split_ratio: fraction for training (rest is val)\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    train_img_dir = os.path.join(output_dir, \"images/train\")\n",
        "    val_img_dir   = os.path.join(output_dir, \"images/val\")\n",
        "    train_lbl_dir = os.path.join(output_dir, \"labels/train\")\n",
        "    val_lbl_dir   = os.path.join(output_dir, \"labels/val\")\n",
        "\n",
        "    for d in [train_img_dir, val_img_dir, train_lbl_dir, val_lbl_dir]:\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "    # Pick all images recursively\n",
        "    all_images = glob.glob(os.path.join(root_images, \"*.jpg\")) + \\\n",
        "                 glob.glob(os.path.join(root_images, \"*.png\")) + \\\n",
        "                 glob.glob(os.path.join(root_images, \"*.jpeg\"))\n",
        "\n",
        "    all_images = [os.path.basename(f) for f in all_images]  # get filenames only\n",
        "    random.shuffle(all_images)\n",
        "    split_index = int(len(all_images)*split_ratio)\n",
        "\n",
        "    train_images = all_images[:split_index]\n",
        "    val_images   = all_images[split_index:]\n",
        "\n",
        "    # Use only 'train' labels folder\n",
        "    labels_folder = os.path.join(root_labels, \"train\")\n",
        "\n",
        "    for img_list, img_dest, lbl_dest in [(train_images, train_img_dir, train_lbl_dir),\n",
        "                                         (val_images, val_img_dir, val_lbl_dir)]:\n",
        "        for img_name in img_list:\n",
        "            lbl_name = os.path.splitext(img_name)[0] + \".txt\"\n",
        "            img_src = os.path.join(root_images, img_name)\n",
        "            lbl_src = os.path.join(labels_folder, lbl_name)\n",
        "\n",
        "            # Copy image\n",
        "            if os.path.exists(img_src):\n",
        "                shutil.copy(img_src, os.path.join(img_dest, img_name))\n",
        "            # Copy label\n",
        "            if os.path.exists(lbl_src):\n",
        "                shutil.copy(lbl_src, os.path.join(lbl_dest, lbl_name))\n",
        "\n",
        "    print(f\"Train images: {len(train_images)}, Val images: {len(val_images)}\")\n",
        "    return train_img_dir, val_img_dir\n",
        "\n",
        "# --------- CrowdHuman Example ----------\n",
        "crowdhuman_images = \"/content/CrowdHuman/images/Images\"\n",
        "crowdhuman_labels = \"/content/CrowdHuman/labels\"\n",
        "crowdhuman_output = \"/content/datasets/CrowdHuman\"\n",
        "\n",
        "train_dir, val_dir = prepare_dataset(crowdhuman_images, crowdhuman_labels, crowdhuman_output)\n",
        "\n",
        "# YAML for CrowdHuman\n",
        "crowdhuman_yaml = {\n",
        "    'train': os.path.join(crowdhuman_output, 'images/train'),\n",
        "    'val': os.path.join(crowdhuman_output, 'images/val'),\n",
        "    'nc': 1,\n",
        "    'names': ['person']\n",
        "}\n",
        "\n",
        "with open(\"/content/crowdhuman.yaml\", \"w\") as f:\n",
        "    yaml.dump(crowdhuman_yaml, f)\n",
        "\n",
        "print(\"✅ CrowdHuman train/val folders and YAML ready!\")"
      ],
      "metadata": {
        "id": "ubgbUphCqCv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, shutil, random, yaml\n",
        "from PIL import Image\n",
        "\n",
        "# Paths\n",
        "images_dir = \"/content/openimages_weapon/data\"\n",
        "labels_json = \"/content/openimages_weapon/labels.json\"\n",
        "output_dir = \"/content/datasets/OpenImages\"\n",
        "split_ratio = 0.8\n",
        "\n",
        "# YOLO classes mapping\n",
        "class_map = {\"Gun\":0, \"Knife\":1, \"Rifle\":2}\n",
        "\n",
        "# Make output folders\n",
        "for d in [\"images/train\", \"images/val\", \"labels/train\", \"labels/val\"]:\n",
        "    os.makedirs(os.path.join(output_dir, d), exist_ok=True)\n",
        "\n",
        "# Load JSON\n",
        "with open(labels_json, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Create mapping: image_id → filename, width, height\n",
        "image_map = {img['id']: {'file_name': img['file_name'], 'width': img['width'], 'height': img['height']} for img in data['images']}\n",
        "\n",
        "all_annotations = []\n",
        "\n",
        "# Iterate over annotations\n",
        "for ann in data['annotations']:\n",
        "    img_id = ann['image_id']\n",
        "    if img_id not in image_map:\n",
        "        continue\n",
        "    info = image_map[img_id]\n",
        "    filename = info['file_name']\n",
        "    img_path = os.path.join(images_dir, filename)\n",
        "    if not os.path.exists(img_path):\n",
        "        continue\n",
        "\n",
        "    category_id = ann['category_id']\n",
        "    cls_name = [c['name'] for c in data['categories'] if c['id']==category_id][0]\n",
        "    if cls_name not in class_map:\n",
        "        continue\n",
        "    class_id = class_map[cls_name]\n",
        "\n",
        "    # COCO bbox: [xmin, ymin, width, height]\n",
        "    xmin, ymin, w, h = ann['bbox']\n",
        "    x_center = (xmin + w/2)/info['width']\n",
        "    y_center = (ymin + h/2)/info['height']\n",
        "    w /= info['width']\n",
        "    h /= info['height']\n",
        "\n",
        "    yolo_lines = [f\"{class_id} {x_center} {y_center} {w} {h}\"]\n",
        "\n",
        "    txt_name = os.path.splitext(filename)[0] + \".txt\"\n",
        "    all_annotations.append({\"filename\": filename, \"yolo_lines\": yolo_lines, \"txt_name\": txt_name})\n",
        "\n",
        "# Shuffle and split\n",
        "random.shuffle(all_annotations)\n",
        "split_index = int(len(all_annotations)*split_ratio)\n",
        "train_annotations = all_annotations[:split_index]\n",
        "val_annotations = all_annotations[split_index:]\n",
        "\n",
        "# Copy images and create YOLO txts\n",
        "for dataset, subset in [(train_annotations, \"train\"), (val_annotations, \"val\")]:\n",
        "    for ann in dataset:\n",
        "        src_img = os.path.join(images_dir, ann['filename'])\n",
        "        dst_img = os.path.join(output_dir, f\"images/{subset}\", ann['filename'])\n",
        "        shutil.copy(src_img, dst_img)\n",
        "\n",
        "        dst_txt = os.path.join(output_dir, f\"labels/{subset}\", ann['txt_name'])\n",
        "        with open(dst_txt, \"w\") as f:\n",
        "            f.write(\"\\n\".join(ann['yolo_lines']))\n",
        "\n",
        "# Create YAML\n",
        "openimages_yaml = {\n",
        "    'train': os.path.join(output_dir, 'images/train'),\n",
        "    'val': os.path.join(output_dir, 'images/val'),\n",
        "    'nc': 3,\n",
        "    'names': ['Gun','Knife','Rifle']\n",
        "}\n",
        "\n",
        "with open(\"/content/openimages_weapon.yaml\", \"w\") as f:\n",
        "    yaml.dump(openimages_yaml, f)\n",
        "\n",
        "print(f\"OpenImages train/val folders and YAML ready! Train: {len(train_annotations)}, Val: {len(val_annotations)}\")"
      ],
      "metadata": {
        "id": "zQTPHXqIxZnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train YOLOv8 on CrowdHuman\n",
        "!yolo task=detect mode=train model=yolov8n.pt \\\n",
        "data=/content/crowdhuman.yaml \\\n",
        "epochs=36 imgsz=640 batch=16 project=/content/yolov8_results name=crowdhuman"
      ],
      "metadata": {
        "id": "4YIfyurLzaBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train YOLOv8 on weapons\n",
        "!yolo task=detect mode=train model=yolov8n.pt \\\n",
        "data=/content/openimages_weapon.yaml \\\n",
        "epochs=50 imgsz=640 batch=16 project=/content/yolov8_results name=openimages_weapon"
      ],
      "metadata": {
        "id": "yKN6ylDVHJpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test person detector\n",
        "!yolo task=detect mode=predict model=/content/yolov8_results/crowdhuman/weights/best.pt \\\n",
        "source=/content/datasets/CrowdHuman/images/val/273271,149030004599ea82.jpg save=True"
      ],
      "metadata": {
        "id": "azAr8NR5Lko7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test weapon detector\n",
        "!yolo task=detect mode=predict model=/content/yolov8_results/openimages_weapon/weights/best.pt \\\n",
        "source=/content/datasets/OpenImages/images/val/01a6ef2ebff837d3.jpg save=True\n"
      ],
      "metadata": {
        "id": "v2Scm9ZSLnCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "\n",
        "# --- Load both models ---\n",
        "person_model = YOLO(\"/content/yolov8_results/crowdhuman/weights/best.pt\")\n",
        "weapon_model = YOLO(\"/content/yolov8_results/openimages_weapon/weights/best.pt\")\n",
        "\n",
        "# --- Input image ---\n",
        "img_path = \"/content/datasets/OpenImages/images/val/016ae9bb1b4cc4be.jpg\"   # yahan apna frame path do\n",
        "image = cv2.imread(img_path)\n",
        "\n",
        "# --- Run Person Detection ---\n",
        "person_results = person_model(img_path, verbose=False)[0]\n",
        "\n",
        "# --- Run Weapon Detection ---\n",
        "weapon_results = weapon_model(img_path, verbose=False)[0]\n",
        "\n",
        "# --- Draw Person Boxes ---\n",
        "for box in person_results.boxes:\n",
        "    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "    conf = float(box.conf[0])\n",
        "    label = f\"Person {conf:.2f}\"\n",
        "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)   # Green box for person\n",
        "    cv2.putText(image, label, (x1, y1-10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "# --- Draw Weapon Boxes ---\n",
        "for box in weapon_results.boxes:\n",
        "    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "    conf = float(box.conf[0])\n",
        "    cls_id = int(box.cls[0])\n",
        "    label = f\"Weapon {weapon_results.names[cls_id]} {conf:.2f}\"\n",
        "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 2)   # Red box for weapon\n",
        "    cv2.putText(image, label, (x1, y1-10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "# --- Save Combined Output ---\n",
        "out_path = \"/content/combined_output3.jpg\"\n",
        "cv2.imwrite(out_path, image)\n",
        "\n",
        "print(f\"✅ Combined result saved at {out_path}\")\n"
      ],
      "metadata": {
        "id": "OtMK0o6iMcGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "\n",
        "# --- Load both models ---\n",
        "person_model = YOLO(\"/content/yolov8_results/crowdhuman/weights/best.pt\")\n",
        "weapon_model = YOLO(\"/content/yolov8_results/openimages_weapon/weights/best.pt\")\n",
        "\n",
        "# --- Input image ---\n",
        "img_path = \"/content/datasets/OpenImages/images/val/01a6ef2ebff837d3.jpg\"   # yahan apna frame path do\n",
        "image = cv2.imread(img_path)\n",
        "\n",
        "# --- Run Person Detection ---\n",
        "person_results = person_model(img_path, verbose=False)[0]\n",
        "\n",
        "# --- Run Weapon Detection ---\n",
        "weapon_results = weapon_model(img_path, verbose=False)[0]\n",
        "\n",
        "# --- Draw Person Boxes ---\n",
        "for box in person_results.boxes:\n",
        "    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "    conf = float(box.conf[0])\n",
        "    label = f\"Person {conf:.2f}\"\n",
        "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)   # Green box for person\n",
        "    cv2.putText(image, label, (x1, y1-10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "# --- Draw Weapon Boxes ---\n",
        "for box in weapon_results.boxes:\n",
        "    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "    conf = float(box.conf[0])\n",
        "    cls_id = int(box.cls[0])\n",
        "    label = f\"Weapon {weapon_results.names[cls_id]} {conf:.2f}\"\n",
        "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 2)   # Red box for weapon\n",
        "    cv2.putText(image, label, (x1, y1-10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "# --- Save Combined Output ---\n",
        "out_path = \"/content/combined_output7.jpg\"\n",
        "cv2.imwrite(out_path, image)\n",
        "\n",
        "print(f\"✅ Combined result saved at {out_path}\")\n"
      ],
      "metadata": {
        "id": "V0PYqjf2M9pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "\n",
        "# Apne dataset folders\n",
        "crowdhuman_dir = \"/content/CrowdHuman/images/Images\"   # CrowdHuman images ka path\n",
        "openimages_dir = \"/content/openimages_weapon/data\"     # OpenImages images ka path\n",
        "\n",
        "# Common frames folder\n",
        "frames_dir = \"/content/frames\"\n",
        "os.makedirs(frames_dir, exist_ok=True)\n",
        "\n",
        "# CrowdHuman images copy (¼ only)\n",
        "ch_files = os.listdir(crowdhuman_dir)\n",
        "ch_limit = len(ch_files) // 4   # 1/4th count\n",
        "for i, file in enumerate(ch_files[:ch_limit]):\n",
        "    src = os.path.join(crowdhuman_dir, file)\n",
        "    dst = os.path.join(frames_dir, f\"ch_{i}.jpg\")\n",
        "    shutil.copy(src, dst)\n",
        "\n",
        "# OpenImages images copy (¼ only)\n",
        "oi_files = os.listdir(openimages_dir)\n",
        "oi_limit = len(oi_files) // 4   # 1/4th count\n",
        "for i, file in enumerate(oi_files[:oi_limit]):\n",
        "    src = os.path.join(openimages_dir, file)\n",
        "    dst = os.path.join(frames_dir, f\"oi_{i}.jpg\")\n",
        "    shutil.copy(src, dst)\n",
        "\n",
        "print(\"CrowdHuman selected:\", ch_limit)\n",
        "print(\"OpenImages selected:\", oi_limit)\n",
        "print(\"Total images in frames folder:\", len(os.listdir(frames_dir)))"
      ],
      "metadata": {
        "id": "dP7vSIv0YUWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- installs (Colab) ---\n",
        "!pip -q install ultralytics scikit-image scikit-learn tqdm\n",
        "\n",
        "import os, cv2, json, random, shutil, time, copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms, models\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from skimage.feature import local_binary_pattern\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# --- paths ---\n",
        "PERSON_W = \"/content/yolov8_results/crowdhuman/weights/best.pt\"\n",
        "WEAPON_W = \"/content/yolov8_results/openimages_weapon/weights/best.pt\"\n",
        "\n",
        "FRAMES_DIR = \"/content/frames\"              # <-- yahan tumhare input frames/images\n",
        "CROPS_DIR  = \"/content/cropped_persons\"     # person crops will be saved here\n",
        "os.makedirs(CROPS_DIR, exist_ok=True)\n",
        "\n",
        "MASTER_CSV = \"/content/frames_labels.csv\"   # image,label (label: 0=Normal,1=WeaponPresent)\n",
        "\n",
        "# subset control for speed (None = use all)\n",
        "MAX_FRAMES_TO_SCAN = 5000     # detect par time bachega\n",
        "MAX_SAMPLES_PER_CLASS = 4000  # later, training subset balance (None to use all)\n",
        "\n",
        "# training config (hybrid CNN+LBPH)\n",
        "IMG_SIZE = 224\n",
        "BATCH = 64\n",
        "EPOCHS = 6\n",
        "LR = 2e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "VAL_SPLIT = 0.15\n",
        "TEST_SPLIT = 0.15\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)"
      ],
      "metadata": {
        "id": "1lxvimINsuZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- paths ---\n",
        "PERSON_W = \"/content/yolov8_results/crowdhuman/weights/best.pt\"\n",
        "WEAPON_W = \"/content/yolov8_results/openimages_weapon/weights/best.pt\"\n",
        "\n",
        "FRAMES_DIR = \"/content/frames\"              # <-- yahan tumhare input frames/images\n",
        "CROPS_DIR  = \"/content/cropped_persons\"     # person crops will be saved here\n",
        "os.makedirs(CROPS_DIR, exist_ok=True)\n",
        "\n",
        "MASTER_CSV = \"/content/frames_labels.csv\"   # image,label (label: 0=Normal,1=WeaponPresent)\n",
        "\n",
        "# subset control for speed (None = use all)\n",
        "MAX_FRAMES_TO_SCAN = 5000     # detect par time bachega\n",
        "MAX_SAMPLES_PER_CLASS = 4000  # later, training subset balance (None to use all)\n",
        "\n",
        "# training config (hybrid CNN+LBPH)\n",
        "IMG_SIZE = 224\n",
        "BATCH = 64\n",
        "EPOCHS = 6\n",
        "LR = 2e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "VAL_SPLIT = 0.15\n",
        "TEST_SPLIT = 0.15\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n"
      ],
      "metadata": {
        "id": "XqVXgXTVsyQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_detection_and_build_csv(frames_dir, crops_dir, out_csv,\n",
        "                                person_w, weapon_w,\n",
        "                                max_frames=None):\n",
        "    person_model = YOLO(person_w)\n",
        "    weapon_model = YOLO(weapon_w)\n",
        "\n",
        "    exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\"}\n",
        "    all_imgs = [f for f in os.listdir(frames_dir) if os.path.splitext(f)[1].lower() in exts]\n",
        "    all_imgs.sort()\n",
        "    if max_frames is not None:\n",
        "        all_imgs = all_imgs[:max_frames]\n",
        "\n",
        "    rows = []\n",
        "    pbar = tqdm(all_imgs, desc=\"Scanning frames\")\n",
        "    for img_name in pbar:\n",
        "        img_path = os.path.join(frames_dir, img_name)\n",
        "        # weapon detect once per frame\n",
        "        wres = weapon_model(img_path, verbose=False)[0]\n",
        "        has_weapon = 1 if len(wres.boxes) > 0 else 0\n",
        "\n",
        "        # person detect\n",
        "        pres = person_model(img_path, verbose=False)[0]\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "        H, W = img.shape[:2]\n",
        "\n",
        "        for i, box in enumerate(pres.boxes):\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "            x1, y1 = max(0,x1), max(0,y1)\n",
        "            x2, y2 = min(W,x2), min(H,y2)\n",
        "            if x2<=x1 or y2<=y1:\n",
        "                continue\n",
        "\n",
        "            crop = img[y1:y2, x1:x2]\n",
        "            crop_name = f\"{os.path.splitext(img_name)[0]}_{i}.jpg\"\n",
        "            crop_path = os.path.join(crops_dir, crop_name)\n",
        "            cv2.imwrite(crop_path, crop)\n",
        "            rows.append([crop_name, has_weapon])\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=[\"image\",\"label\"])\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"✅ CSV saved: {out_csv} | samples: {len(df)}\")\n",
        "\n",
        "# Run once (skip if CSV already exists)\n",
        "if not os.path.exists(MASTER_CSV):\n",
        "    run_detection_and_build_csv(FRAMES_DIR, CROPS_DIR, MASTER_CSV,\n",
        "                                PERSON_W, WEAPON_W,\n",
        "                                max_frames=MAX_FRAMES_TO_SCAN)\n",
        "else:\n",
        "    print(\"CSV already exists:\", MASTER_CSV)"
      ],
      "metadata": {
        "id": "AVVOPkZesyNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_optionally_balance(csv_path, crops_dir, max_per_class=None, seed=42):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # remove missing crops\n",
        "    df[\"exists\"] = df[\"image\"].apply(lambda x: os.path.exists(os.path.join(crops_dir, x)))\n",
        "    df = df[df[\"exists\"]].drop(columns=[\"exists\"]).reset_index(drop=True)\n",
        "\n",
        "    if max_per_class is not None:\n",
        "        dfs = []\n",
        "        for lbl, grp in df.groupby(\"label\"):\n",
        "            grp = grp.sample(n=min(max_per_class, len(grp)), random_state=seed)\n",
        "            dfs.append(grp)\n",
        "        df = pd.concat(dfs).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "    print(\"Class counts:\\n\", df[\"label\"].value_counts())\n",
        "    return df\n",
        "\n",
        "df_all = clean_and_optionally_balance(MASTER_CSV, CROPS_DIR, max_per_class=MAX_SAMPLES_PER_CLASS, seed=SEED)\n",
        "print(\"Total samples:\", len(df_all))\n"
      ],
      "metadata": {
        "id": "YEycclQ-syKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, temp_df = train_test_split(df_all, test_size=VAL_SPLIT+TEST_SPLIT,\n",
        "                                     stratify=df_all[\"label\"], random_state=SEED)\n",
        "rel_test = TEST_SPLIT / (VAL_SPLIT+TEST_SPLIT + 1e-9)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=rel_test,\n",
        "                                   stratify=temp_df[\"label\"], random_state=SEED)\n",
        "\n",
        "print(\"Train:\", train_df.shape, \"| Val:\", val_df.shape, \"| Test:\", test_df.shape)\n",
        "print(\"Train dist:\\n\", train_df[\"label\"].value_counts(), \"\\nVal dist:\\n\", val_df[\"label\"].value_counts(), \"\\nTest dist:\\n\", test_df[\"label\"].value_counts())\n",
        "\n",
        "# Save splits\n",
        "train_df.to_csv(\"/content/train_labels.csv\", index=False)\n",
        "val_df.to_csv(\"/content/val_labels.csv\", index=False)\n",
        "test_df.to_csv(\"/content/test_labels.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "pGCp5uEpsyHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LBPH extractor\n",
        "def lbph_feature(img_path, radius=1, n_points=8):\n",
        "    g = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if g is None:\n",
        "        # fallback: tiny zero vec (should be rare if paths are cleaned)\n",
        "        return np.zeros(n_points+2, dtype=np.float32)\n",
        "    lbp = local_binary_pattern(g, n_points, radius, method=\"uniform\")\n",
        "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points+3), range=(0, n_points+2))\n",
        "    hist = hist.astype(\"float32\")\n",
        "    hist /= (hist.sum() + 1e-6)\n",
        "    return hist\n",
        "\n",
        "class PersonHybridDataset(Dataset):\n",
        "    def __init__(self, df, crops_dir, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.crops_dir = crops_dir\n",
        "        self.transform = transform\n",
        "        self._lbph_cache = {}  # simple memory cache\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.crops_dir, row[\"image\"])\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform: img = self.transform(img)\n",
        "\n",
        "        if img_path not in self._lbph_cache:\n",
        "            self._lbph_cache[img_path] = lbph_feature(img_path)\n",
        "        lbph = torch.tensor(self._lbph_cache[img_path], dtype=torch.float32)\n",
        "\n",
        "        label = torch.tensor(int(row[\"label\"]), dtype=torch.float32)\n",
        "        return img, lbph, label\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(0.2,0.2,0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "eval_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "train_ds = PersonHybridDataset(train_df, CROPS_DIR, transform=train_tf)\n",
        "val_ds   = PersonHybridDataset(val_df,   CROPS_DIR, transform=eval_tf)\n",
        "test_ds  = PersonHybridDataset(test_df,  CROPS_DIR, transform=eval_tf)\n",
        "\n",
        "# handle imbalance (weights per class)\n",
        "counts = train_df[\"label\"].value_counts().to_dict()\n",
        "w = {0: 1.0/max(counts.get(0,1),1), 1: 1.0/max(counts.get(1,1),1)}\n",
        "sample_w = train_df[\"label\"].map(w).values\n",
        "sampler = WeightedRandomSampler(sample_w, num_samples=len(sample_w), replacement=True)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH, sampler=sampler, num_workers=0, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=True)\n",
        "print(\"✅ DataLoaders ready\")"
      ],
      "metadata": {
        "id": "mtjlT118syFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_LBPH(nn.Module):\n",
        "    def __init__(self, lbph_dim=10, num_classes=1, freeze_backbone=False):\n",
        "        super().__init__()\n",
        "        self.cnn = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        if freeze_backbone:\n",
        "            for p in self.cnn.parameters(): p.requires_grad = False\n",
        "        # replace final with identity to get 512-d\n",
        "        self.cnn.fc = nn.Identity()\n",
        "        self.lbph_fc = nn.Sequential(\n",
        "            nn.Linear(lbph_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 + 64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, num_classes)  # logits\n",
        "        )\n",
        "\n",
        "    def forward(self, x_img, x_lbph):\n",
        "        f_img = self.cnn(x_img)               # [B,512]\n",
        "        f_lb  = self.lbph_fc(x_lbph)          # [B,64]\n",
        "        f = torch.cat([f_img, f_lb], dim=1)   # [B,576]\n",
        "        logit = self.classifier(f).squeeze(1) # [B]\n",
        "        return logit\n",
        "\n",
        "model = CNN_LBPH(lbph_dim=10, num_classes=1, freeze_backbone=False).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "scaler = torch.amp.GradScaler('cuda') if torch.cuda.is_available() else None"
      ],
      "metadata": {
        "id": "_3LVv02fsyDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_epoch(loader, train=True):\n",
        "    model.train(train)\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    y_true, y_prob = [], []\n",
        "\n",
        "    for imgs, lbph, labels in tqdm(loader, leave=False):\n",
        "        imgs, lbph, labels = imgs.to(device), lbph.to(device), labels.to(device)\n",
        "\n",
        "        with torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n",
        "            logits = model(imgs, lbph)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            if scaler:\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        loss_sum += loss.item() * imgs.size(0)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = (probs > 0.5).float()\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total   += imgs.size(0)\n",
        "\n",
        "        y_true += labels.detach().cpu().numpy().tolist()\n",
        "        y_prob += probs.detach().cpu().numpy().tolist()\n",
        "\n",
        "    acc = correct / max(total,1)\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_prob)\n",
        "    except:\n",
        "        auc = 0.0\n",
        "    return loss_sum/max(total,1), acc, auc\n",
        "\n",
        "best_state, best_metric = None, -1\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc, tr_auc = run_epoch(train_loader, train=True)\n",
        "    val_loss, val_acc, val_auc = run_epoch(val_loader, train=False)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"[{ep}/{EPOCHS}] \"\n",
        "          f\"train loss {tr_loss:.4f} acc {tr_acc:.3f} auc {tr_auc:.3f} | \"\n",
        "          f\"val loss {val_loss:.4f} acc {val_acc:.3f} auc {val_auc:.3f}\")\n",
        "\n",
        "    score = val_auc if val_auc>0 else val_acc\n",
        "    if score > best_metric:\n",
        "        best_metric = score\n",
        "        best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "# save best\n",
        "os.makedirs(\"/content/checkpoints\", exist_ok=True)\n",
        "if best_state: model.load_state_dict(best_state)\n",
        "torch.save(model.state_dict(), \"/content/checkpoints/cnn_lbph_best.pt\")\n",
        "print(\"✅ Saved:\", \"/content/checkpoints/cnn_lbph_best.pt\")"
      ],
      "metadata": {
        "id": "rGvk7JztsyA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "y_true, y_pred, y_prob = [], [], []\n",
        "with torch.no_grad():\n",
        "    for imgs, lbph, labels in tqdm(test_loader):\n",
        "        imgs, lbph = imgs.to(device), lbph.to(device)\n",
        "        logits = model(imgs, lbph)\n",
        "        probs  = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds  = (probs > 0.5).astype(int)\n",
        "        y_true.extend(labels.numpy().tolist())\n",
        "        y_pred.extend(preds.tolist())\n",
        "        y_prob.extend(probs.tolist())\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Normal\",\"Weapon/Suspicious\"]))\n",
        "print(\"Test AUC:\", roc_auc_score(y_true, y_prob))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))"
      ],
      "metadata": {
        "id": "XTSIGHROsx-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def annotate_and_alert(image_path, person_w, weapon_w, clf_model, crops_tmp=\"/content/tmp_crops\"):\n",
        "    os.makedirs(crops_tmp, exist_ok=True)\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(\"Could not read:\", image_path)\n",
        "        return None, None\n",
        "\n",
        "    H,W = img.shape[:2]\n",
        "    person_model = YOLO(person_w)\n",
        "    weapon_model = YOLO(weapon_w)\n",
        "\n",
        "    pres = person_model(image_path, verbose=False)[0]\n",
        "    wres = weapon_model(image_path, verbose=False)[0]\n",
        "\n",
        "    has_weapon = len(wres.boxes) > 0\n",
        "\n",
        "    # draw weapon boxes (red)\n",
        "    for b in wres.boxes:\n",
        "        x1,y1,x2,y2 = map(int, b.xyxy[0])\n",
        "        cv2.rectangle(img, (x1,y1), (x2,y2), (0,0,255), 2)\n",
        "        cv2.putText(img, f\"Weapon {float(b.conf[0]):.2f}\", (x1, max(10,y1-5)),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,255), 2)\n",
        "\n",
        "    suspicious_any = False\n",
        "    # per-person classify (green = normal, yellow = suspicious)\n",
        "    for i, b in enumerate(pres.boxes):\n",
        "        x1,y1,x2,y2 = map(int, b.xyxy[0])\n",
        "        x1,y1 = max(0,x1), max(0,y1)\n",
        "        x2,y2 = min(W,x2), min(H,y2)\n",
        "        if x2<=x1 or y2<=y1: continue\n",
        "\n",
        "        crop = img[y1:y2, x1:x2]\n",
        "        tmp_path = os.path.join(crops_tmp, f\"tmp_{i}.jpg\")\n",
        "        cv2.imwrite(tmp_path, crop)\n",
        "\n",
        "        # make tensor + lbph\n",
        "        pil = Image.open(tmp_path).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n",
        "        tensor = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "        ])(pil).unsqueeze(0).to(device)\n",
        "\n",
        "        lbph = torch.tensor(lbph_feature(tmp_path), dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad(), torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n",
        "            logit = clf_model(tensor, lbph)\n",
        "            prob = torch.sigmoid(logit).item()\n",
        "        is_susp = prob > 0.5\n",
        "        suspicious_any = suspicious_any or is_susp\n",
        "\n",
        "        color = (0,255,0) if not is_susp else (0,255,255)\n",
        "        tag   = \"Normal\" if not is_susp else f\"Suspicious {prob:.2f}\"\n",
        "        cv2.rectangle(img, (x1,y1), (x2,y2), color, 2)\n",
        "        cv2.putText(img, tag, (x1, max(10,y1-5)),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "    intruder_alert = has_weapon or suspicious_any\n",
        "    status = \"INTRUDER ALERT\" if intruder_alert else \"CLEAR\"\n",
        "    cv2.putText(img, status, (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1.0,\n",
        "                (0,0,255) if intruder_alert else (0,255,0), 3)\n",
        "\n",
        "    return img, intruder_alert\n",
        "\n",
        "# --- load trained hybrid model and test on a sample image ---\n",
        "clf = CNN_LBPH(lbph_dim=10, num_classes=1, freeze_backbone=False).to(device)\n",
        "clf.load_state_dict(torch.load(\"/content/checkpoints/cnn_lbph_best.pt\", map_location=device))\n",
        "clf.eval()\n",
        "\n",
        "test_image = os.path.join(FRAMES_DIR, sorted(os.listdir(FRAMES_DIR))[0])  # pick any frame\n",
        "out_img, alert = annotate_and_alert(test_image, PERSON_W, WEAPON_W, clf)\n",
        "out_path = \"/content/annotated_result1.jpg\"\n",
        "if out_img is not None:\n",
        "    cv2.imwrite(out_path, out_img)\n",
        "    print(f\"✅ Saved annotated image: {out_path} | Alert = {alert}\")"
      ],
      "metadata": {
        "id": "4ZnddtxPwkCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MqYYKwGpwj7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jQ7NQTkhwj5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lc1gzXJLzwAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iygBE_tjzv84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GaOOatpGzv6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EgIB3VGjzv3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7aYD4VdTzv1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A3WckVJOzvzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2\n",
        "from ultralytics import YOLO\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Paths\n",
        "person_model_path = \"/content/yolov8_results/crowdhuman/weights/best.pt\"\n",
        "weapon_model_path = \"/content/yolov8_results/openimages_weapon/weights/best.pt\"\n",
        "frames_dir = \"/content/frames\"\n",
        "output_dir = \"/content/cropped_persons1\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load models\n",
        "person_model = YOLO(person_model_path)\n",
        "weapon_model = YOLO(weapon_model_path)\n",
        "\n",
        "# Get images (subset of 5000 first)\n",
        "all_images = os.listdir(frames_dir)\n",
        "random.shuffle(all_images)\n",
        "subset_images = all_images[:5000]   # ⚡ change 5000 → 21731 for full dataset\n",
        "\n",
        "labels = []\n",
        "\n",
        "# Iterate with progress bar\n",
        "for img_name in tqdm(subset_images):\n",
        "    img_path = os.path.join(frames_dir, img_name)\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    # Detect persons\n",
        "    person_results = person_model(img_path, verbose=False)[0]\n",
        "\n",
        "    # Detect weapons\n",
        "    weapon_results = weapon_model(img_path, verbose=False)[0]\n",
        "\n",
        "    # Label (1 if weapon detected, else 0)\n",
        "    label = 1 if len(weapon_results.boxes) > 0 else 0\n",
        "\n",
        "    # Crop persons\n",
        "    for i, box in enumerate(person_results.boxes):\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "        cropped = img[y1:y2, x1:x2]\n",
        "        crop_name = f\"{os.path.splitext(img_name)[0]}_{i}.jpg\"\n",
        "        crop_path = os.path.join(output_dir, crop_name)\n",
        "        cv2.imwrite(crop_path, cropped)\n",
        "\n",
        "        # Store row for CSV\n",
        "        labels.append([crop_name, label])\n",
        "\n",
        "# Save CSV once\n",
        "df = pd.DataFrame(labels, columns=[\"image\", \"label\"])\n",
        "df.to_csv(\"frames_labels.csv\", index=False)\n",
        "print(\"✅ Done. Total samples:\", len(df))"
      ],
      "metadata": {
        "id": "Qmz2QfbdTeWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil, os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load CSV, skipping the header row\n",
        "df = pd.read_csv(\"frames_labels.csv\", skiprows=1, names=[\"image\", \"label\"])\n",
        "\n",
        "# Split\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df[\"label\"], random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42)\n",
        "\n",
        "# Save splits\n",
        "train_df.to_csv(\"train_labels.csv\", index=False)\n",
        "val_df.to_csv(\"val_labels.csv\", index=False)\n",
        "test_df.to_csv(\"test_labels.csv\", index=False)\n",
        "\n",
        "print(\"Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))"
      ],
      "metadata": {
        "id": "Q1dFZikDTeT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train distribution:\\n\", train_df[\"label\"].value_counts())\n",
        "print(\"Val distribution:\\n\", val_df[\"label\"].value_counts())\n",
        "print(\"Test distribution:\\n\", test_df[\"label\"].value_counts())\n"
      ],
      "metadata": {
        "id": "Yu2V4Qg9jopd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load full CSV\n",
        "df = pd.read_csv(\"frames_labels.csv\", skiprows=1, names=[\"image\",\"label\"])\n",
        "\n",
        "# Separate classes\n",
        "df_normal = df[df[\"label\"]==0]\n",
        "df_weapon = df[df[\"label\"]==1]\n",
        "\n",
        "# Decide subset size\n",
        "n_weapon = len(df_weapon)          # total weapon images\n",
        "n_normal = n_weapon                # same number of normal images for balance\n",
        "\n",
        "df_normal_sub = df_normal.sample(n=n_normal, random_state=42)\n",
        "df_weapon_sub = df_weapon.sample(n=n_weapon, random_state=42)\n",
        "\n",
        "# Combine\n",
        "df_sub = pd.concat([df_normal_sub, df_weapon_sub]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split train/val/test\n",
        "train_df, temp_df = train_test_split(df_sub, test_size=0.3, stratify=df_sub[\"label\"], random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42)\n",
        "\n",
        "# Save CSVs\n",
        "train_df.to_csv(\"train_labels_subset.csv\", index=False)\n",
        "val_df.to_csv(\"val_labels_subset.csv\", index=False)\n",
        "test_df.to_csv(\"test_labels_subset.csv\", index=False)\n",
        "\n",
        "print(\"Subset Train:\", len(train_df))\n",
        "print(\"Subset Val:\", len(val_df))\n",
        "print(\"Subset Test:\", len(test_df))\n",
        "\n",
        "print(\"\\nTrain distribution:\\n\", train_df[\"label\"].value_counts())\n",
        "print(\"Val distribution:\\n\", val_df[\"label\"].value_counts())\n",
        "print(\"Test distribution:\\n\", test_df[\"label\"].value_counts())\n"
      ],
      "metadata": {
        "id": "tPbYQYo0j17g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# ---------- Transforms ----------\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "eval_tf = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "# ---------- Dataset ----------\n",
        "class FrameCSV(Dataset):\n",
        "    def __init__(self, csv_path, transform=None):\n",
        "        import pandas as pd\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row[\"image\"]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = int(row[\"label\"])\n",
        "        return img, label\n",
        "\n",
        "# ---------- Load subset datasets ----------\n",
        "train_ds = FrameCSV(\"train_labels_subset.csv\", transform=train_tf)\n",
        "val_ds   = FrameCSV(\"val_labels_subset.csv\", transform=eval_tf)\n",
        "test_ds  = FrameCSV(\"test_labels_subset.csv\", transform=eval_tf)\n",
        "\n",
        "# ---------- Weighted sampler for train ----------\n",
        "labels = train_ds.df[\"label\"].values\n",
        "class_sample_count = torch.bincount(torch.tensor(labels))\n",
        "weights = 1.0 / class_sample_count\n",
        "sample_weights = weights[labels]\n",
        "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "# ---------- DataLoaders ----------\n",
        "BATCH = 32\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH, sampler=sampler, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"✅ DataLoaders ready with subset!\")\n"
      ],
      "metadata": {
        "id": "x_llo8uVkEIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import copy\n",
        "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------- Dataset ----------\n",
        "class FrameDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row['image']).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = int(row['label'])\n",
        "        return img, label\n",
        "\n",
        "# ---------- Transforms ----------\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "eval_tf = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# ---------- Datasets ----------\n",
        "train_ds = FrameDataset(\"train_labels_subset.csv\", transform=train_tf)\n",
        "val_ds   = FrameDataset(\"val_labels_subset.csv\",   transform=eval_tf)\n",
        "test_ds  = FrameDataset(\"test_labels_subset.csv\",  transform=eval_tf)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# ---------- Model ----------\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n"
      ],
      "metadata": {
        "id": "QPoMcfoukWhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import amp\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    model.train(train)\n",
        "    loss_sum, correct, n = 0.0, 0, 0\n",
        "    all_probs, all_labels = [], []\n",
        "\n",
        "    scaler = amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "    for imgs, labels in tqdm(loader, leave=False):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        with amp.autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        loss_sum += loss.item() * imgs.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        n += imgs.size(0)\n",
        "\n",
        "        all_probs += torch.softmax(logits,1)[:,1].detach().cpu().tolist()\n",
        "        all_labels += labels.detach().cpu().tolist()\n",
        "\n",
        "    auc = 0.0\n",
        "    try:\n",
        "        auc = roc_auc_score(all_labels, all_probs)\n",
        "    except: pass\n",
        "    return loss_sum/n, correct/n, auc\n"
      ],
      "metadata": {
        "id": "xiX7kw8BkXjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, copy, pandas as pd, numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms, models\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ---------------- Device ----------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------- Dataset ----------------\n",
        "class FrameDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.root_dir, row['image'])  # full path fix\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = int(row['label'])\n",
        "        return img, label\n",
        "\n",
        "# ---------------- Transforms ----------------\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(0.2,0.2,0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "eval_tf = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# ---------------- Paths ----------------\n",
        "root_dir = \"/content/cropped_persons1\"  # cropped persons folder\n",
        "train_csv = \"train_labels_subset.csv\"\n",
        "val_csv   = \"val_labels_subset.csv\"\n",
        "test_csv  = \"test_labels_subset.csv\"\n",
        "\n",
        "train_ds = FrameDataset(train_csv, root_dir, transform=train_tf)\n",
        "val_ds   = FrameDataset(val_csv, root_dir, transform=eval_tf)\n",
        "test_ds  = FrameDataset(test_csv, root_dir, transform=eval_tf)\n",
        "\n",
        "# ---------------- Weighted Sampler for imbalance ----------------\n",
        "labels = pd.read_csv(train_csv)[\"label\"].values\n",
        "class_sample_count = np.bincount(labels)\n",
        "weights = 1.0 / class_sample_count\n",
        "sample_weights = weights[labels]\n",
        "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "# ---------------- DataLoaders ----------------\n",
        "BATCH = 32\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH, sampler=sampler, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# ---------------- Model ----------------\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)  # 2-class\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "# ---------------- Train/Eval loop ----------------\n",
        "def run_epoch(loader, train=True):\n",
        "    model.train(train)\n",
        "    loss_sum, correct, n = 0.0, 0, 0\n",
        "    all_probs, all_labels = [], []\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "    for imgs, labels in tqdm(loader, leave=False):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        loss_sum += loss.item() * imgs.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        n += imgs.size(0)\n",
        "        all_probs += torch.softmax(logits,1)[:,1].detach().cpu().tolist()\n",
        "        all_labels += labels.detach().cpu().tolist()\n",
        "\n",
        "    auc = 0.0\n",
        "    try: auc = roc_auc_score(all_labels, all_probs)\n",
        "    except: pass\n",
        "    return loss_sum/n, correct/n, auc\n",
        "\n",
        "# ---------------- Training ----------------\n",
        "best_wts, best_val = None, 0.0\n",
        "EPOCHS = 8\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc, tr_auc = run_epoch(train_loader, train=True)\n",
        "    val_loss, val_acc, val_auc = run_epoch(val_loader, train=False)\n",
        "    scheduler.step()\n",
        "    print(f\"[{ep}/{EPOCHS}] train loss {tr_loss:.4f} acc {tr_acc:.3f} auc {tr_auc:.3f} | \"\n",
        "          f\"val loss {val_loss:.4f} acc {val_acc:.3f} auc {val_auc:.3f}\")\n",
        "\n",
        "    score = val_auc if val_auc>0 else val_acc\n",
        "    if score > best_val:\n",
        "        best_val = score\n",
        "        best_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "# Load best weights\n",
        "if best_wts: model.load_state_dict(best_wts)\n",
        "os.makedirs(\"/content/checkpoints\", exist_ok=True)\n",
        "torch.save(model.state_dict(), \"/content/checkpoints/resnet18_subset_best.pt\")\n",
        "print(\"✅ Saved -> /content/checkpoints/resnet18_subset_best.pt\")\n",
        "\n",
        "# ---------------- Test Metrics ----------------\n",
        "model.eval()\n",
        "y_true, y_pred, y_prob = [], [], []\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        logits = model(imgs)\n",
        "        y_true += labels.cpu().tolist()\n",
        "        y_pred += logits.argmax(1).cpu().tolist()\n",
        "        y_prob += torch.softmax(logits,1)[:,1].cpu().tolist()\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Normal\",\"Weapon\"]))\n",
        "print(\"Test AUC:\", roc_auc_score(y_true, y_prob))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "SRxgG12FkXdZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}